diff --git a/scripts/train.py b/scripts/train.py
index e4b7af9..7bf5d3c 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -5,121 +5,134 @@ from maple.launchers.robosuite_launcher import experiment
 import maple.util.hyperparameter as hyp
 import collections
 
-base_variant = dict(
-    layer_size=256,
-    replay_buffer_size=int(1E6),
-    rollout_fn_kwargs=dict(
-        terminals_all_false=True,
-    ),
-    algorithm_kwargs=dict(
-        num_epochs=10000,
-        num_expl_steps_per_train_loop=3000,
-        num_eval_steps_per_epoch=3000,
-        num_trains_per_train_loop=1000,
-        min_num_steps_before_training=30000,
-        max_path_length=150,
-        batch_size=1024,
-        eval_epoch_freq=10,
-    ),
-    trainer_kwargs=dict(
-        discount=0.99,
-        soft_target_tau=1e-3,
-        target_update_period=1,
-        policy_lr=3e-5,
-        qf_lr=3e-5,
-        reward_scale=1,
-        use_automatic_entropy_tuning=True,
-    ),
-    ll_sac_variant=dict(
-        high_init_ent=True,
-    ),
-    pamdp_variant=dict(
-        one_hot_s=True,
-        high_init_ent=True,
-        one_hot_factor=0.50,
-    ),
-    env_variant=dict(
-        robot_keys=['robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'robot0_gripper_qvel'],
-        obj_keys=['object-state'],
-        controller_type='OSC_POSITION_YAW',
-        controller_config_update=dict(
-            position_limits=[
-                [-0.30, -0.30, 0.75],
-                [0.15, 0.30, 1.15]
-            ],
-        ),
-        env_kwargs=dict(
-            ignore_done=True,
-            reward_shaping=True,
-            hard_reset=False,
-            control_freq=10,
-            camera_heights=512,
-            camera_widths=512,
-            table_offset=[-0.075, 0, 0.8],
-            reward_scale=5.0,
+# base_variant = dict(
+#     layer_size=256,
+#     replay_buffer_size=int(1E6),
+#     rollout_fn_kwargs=dict(
+#         terminals_all_false=True,
+#     ),
+#     algorithm_kwargs=dict(
+#         num_epochs=10000,
+#         num_expl_steps_per_train_loop=3000,
+#         num_eval_steps_per_epoch=3000,
+#         num_trains_per_train_loop=1000,
+#         min_num_steps_before_training=30000,
+#         max_path_length=150,
+#         batch_size=1024,
+#         eval_epoch_freq=10,
+#     ),
+#     trainer_kwargs=dict(
+#         discount=0.99,
+#         soft_target_tau=1e-3,
+#         target_update_period=1,
+#         policy_lr=3e-5,
+#         qf_lr=3e-5,
+#         reward_scale=1,
+#         use_automatic_entropy_tuning=True,
+#     ),
+#     ll_sac_variant=dict(
+#         high_init_ent=True,
+#     ),
+#     pamdp_variant=dict(
+#         one_hot_s=True,
+#         high_init_ent=True,
+#         one_hot_factor=0.50,
+#     ),
+#     env_variant=dict(
+#         robot_keys=['robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'robot0_gripper_qvel'],
+#         obj_keys=['object-state'],
+#         controller_type='OSC_POSITION_YAW',
+#         controller_config_update=dict(
+#             position_limits=[
+#                 [-0.30, -0.30, 0.75],
+#                 [0.15, 0.30, 1.15]
+#             ],
+#         ),
+#         env_kwargs=dict(
+#             ignore_done=True,
+#             reward_shaping=True,
+#             hard_reset=False,
+#             control_freq=10,
+#             camera_heights=512,
+#             camera_widths=512,
+#             table_offset=[-0.075, 0, 0.8],
+#             reward_scale=5.0,
 
-            skill_config=dict(
-                skills=['atomic', 'open', 'reach', 'grasp', 'push'],
-                aff_penalty_fac=15.0,
+#             skill_config=dict(
+#                 skills=['atomic', 'open', 'reach', 'grasp', 'push'],
+#                 aff_penalty_fac=15.0,
 
-                base_config=dict(
-                    global_xyz_bounds=[
-                        [-0.30, -0.30, 0.80],
-                        [0.15, 0.30, 0.95]
-                    ],
-                    lift_height=0.95,
-                    binary_gripper=True,
+#                 base_config=dict(
+#                     global_xyz_bounds=[
+#                         [-0.30, -0.30, 0.80],
+#                         [0.15, 0.30, 0.95]
+#                     ],
+#                     lift_height=0.95,
+#                     binary_gripper=True,
 
-                    aff_threshold=0.06,
-                    aff_type='dense',
-                    aff_tanh_scaling=10.0,
-                ),
-                atomic_config=dict(
-                    use_ori_params=True,
-                ),
-                reach_config=dict(
-                    use_gripper_params=False,
-                    local_xyz_scale=[0.0, 0.0, 0.06],
-                    use_ori_params=False,
-                    max_ac_calls=15,
-                ),
-                grasp_config=dict(
-                    global_xyz_bounds=[
-                        [-0.30, -0.30, 0.80],
-                        [0.15, 0.30, 0.85]
-                    ],
-                    aff_threshold=0.03,
+#                     aff_threshold=0.06,
+#                     aff_type='dense',
+#                     aff_tanh_scaling=10.0,
+#                 ),
+#                 atomic_config=dict(
+#                     use_ori_params=True,
+#                 ),
+#                 reach_config=dict(
+#                     use_gripper_params=False,
+#                     local_xyz_scale=[0.0, 0.0, 0.06],
+#                     use_ori_params=False,
+#                     max_ac_calls=15,
+#                 ),
+#                 grasp_config=dict(
+#                     global_xyz_bounds=[
+#                         [-0.30, -0.30, 0.80],
+#                         [0.15, 0.30, 0.85]
+#                     ],
+#                     aff_threshold=0.03,
 
-                    local_xyz_scale=[0.0, 0.0, 0.0],
-                    use_ori_params=True,
-                    max_ac_calls=20,
-                    num_reach_steps=2,
-                    num_grasp_steps=3,
-                ),
-                push_config=dict(
-                    global_xyz_bounds=[
-                        [-0.30, -0.30, 0.80],
-                        [0.15, 0.30, 0.85]
-                    ],
-                    delta_xyz_scale=[0.25, 0.25, 0.05],
+#                     local_xyz_scale=[0.0, 0.0, 0.0],
+#                     use_ori_params=True,
+#                     max_ac_calls=20,
+#                     num_reach_steps=2,
+#                     num_grasp_steps=3,
+#                 ),
+#                 push_config=dict(
+#                     global_xyz_bounds=[
+#                         [-0.30, -0.30, 0.80],
+#                         [0.15, 0.30, 0.85]
+#                     ],
+#                     delta_xyz_scale=[0.25, 0.25, 0.05],
 
-                    max_ac_calls=20,
-                    use_ori_params=True,
+#                     max_ac_calls=20,
+#                     use_ori_params=True,
+
+#                     aff_threshold=[0.12, 0.12, 0.04],
+#                 ),
+#             ),
+#         ),
+#     ),
+#     save_video=True,
+#     save_video_period=100,
+#     dump_video_kwargs=dict(
+#         rows=1,
+#         columns=6,
+#         pad_length=5,
+#         pad_color=0,
+#     ),
+# )
+
+
+base_variant = {
+    'env_variant': {
+        'skill_config': {
+            'grasp_config': {
+                'global_xyz_bounds': [[0, 0, 0], [1, 1, 1]],
+                'aff_threshold': 0.1
+            }
+        }
+    }
+}
 
-                    aff_threshold=[0.12, 0.12, 0.04],
-                ),
-            ),
-        ),
-    ),
-    save_video=True,
-    save_video_period=100,
-    dump_video_kwargs=dict(
-        rows=1,
-        columns=6,
-        pad_length=5,
-        pad_color=0,
-    ),
-)
 
 env_params = dict(
     lift={
@@ -201,7 +214,7 @@ def process_variant(variant):
         variant['algorithm_kwargs']['min_num_steps_before_training'] = steps
         variant['algorithm_kwargs']['num_trains_per_train_loop'] = 50
         variant['replay_buffer_size'] = int(1E3)
-        variant['dump_video_kwargs']['columns'] = 2
+        variant['dump_video_kwargs']['columns'] = 3
 
     if args.no_video:
         variant['save_video'] = False
@@ -248,7 +261,7 @@ if __name__ == "__main__":
             experiment,
             exp_folder=args.env,
             exp_prefix=args.label,
-            variant=variant,
+            variant=base_variant,
             snapshot_mode='gap_and_last',
             snapshot_gap=args.snapshot_gap,
             exp_id=exp_id,
@@ -260,3 +273,10 @@ if __name__ == "__main__":
 
         if args.first_variant:
             exit()
+
+
+# To do:
+# Understand what the videos in data mean
+# Just change debug mode parameters: Run wipe for 1 epoch and see output (maybe even force it to execute reach for specific (x,y,z))
+# Change controller and see what parameters are needed
+# Update parameters based on 1 epoch output so that it now completely works with impedance controller and uses impedance parameters
\ No newline at end of file
